# üìã Respuestas a las Preguntas sobre la Historia

## 1. ¬øPor qu√© es peligroso practicar con personas reales?

**Respuesta:**

Cuando decimos "practicar con personas reales", nos referimos a que el agente RL aprenda **en vivo** usando datos del d√≠a a d√≠a de hoy, tomando decisiones reales sobre pr√©stamos de clientes reales mientras est√° aprendiendo.

**¬øPor qu√© es peligroso?**

1. **Riesgo financiero**: Si el agente toma malas decisiones mientras aprende, puede:
   - Aprobar pr√©stamos que deber√≠an rechazarse ‚Üí p√©rdidas financieras
   - Rechazar pr√©stamos buenos ‚Üí p√©rdida de oportunidades
   - Contactar HQ innecesariamente ‚Üí costos operativos

2. **Riesgo √©tico**: No podemos usar clientes reales como "conejillos de indias" para experimentar con pol√≠ticas que a√∫n no est√°n validadas.

3. **Riesgo regulatorio**: Los bancos tienen regulaciones estrictas sobre c√≥mo se toman decisiones crediticias. Un agente que aprende en vivo podr√≠a violar estas regulaciones.

**Por eso usamos "Offline RL":**
- El agente aprende de **datos hist√≥ricos** (eventos pasados que ya ocurrieron)
- No toma decisiones reales mientras aprende
- Solo despu√©s de validar que funciona bien (con OPE), se despliega

**C√≥digo relacionado:**
- `src/xppm/rl/train_tdqn.py` - Entrenamiento offline (no interacci√≥n en vivo)
- `src/xppm/ope/doubly_robust.py` - Evaluaci√≥n sin interacci√≥n (OPE)

---

## 2. ¬øQu√© parte del c√≥digo hace decisiones m√∫ltiples y d√≥nde se guardan?

**Respuesta:**

El robot toma **m√∫ltiples decisiones a lo largo del tiempo** para cada caso (pr√©stamo). Esto se implementa en:

### C√≥digo que implementa decisiones m√∫ltiples:

1. **Construcci√≥n del dataset MDP** (`src/xppm/data/build_mdp.py`):
   - L√≠neas 269-433: `build_transitions()` crea m√∫ltiples transiciones por caso
   - Cada prefijo (momento en el tiempo) es un **punto de decisi√≥n**
   - Para cada caso, se generan transiciones en m√∫ltiples pasos temporales (`t=1, t=2, ..., t=L`)

2. **Entrenamiento TDQN** (`src/xppm/rl/train_tdqn.py`):
   - El modelo aprende `Q(s_t, a)` para cada estado `s_t` en la secuencia
   - En cada paso temporal, el modelo puede decidir una acci√≥n diferente
   - La pol√≠tica aprendida es **secuencial**: decisiones en t=1 afectan decisiones en t=2, t=3, etc.

3. **Explicaciones** (`src/xppm/xai/explain_policy.py`):
   - Selecciona m√∫ltiples transiciones por caso (l√≠neas 324-339)
   - Genera explicaciones para cada momento de decisi√≥n
   - Puede explicar por qu√© decidi√≥ X en t=5 y Y en t=10

### D√≥nde se guardan los resultados:

**Dataset MDP:**
- `data/processed/D_offline.npz` - Contiene todas las transiciones (s, a, r, s_next)
- Cada transici√≥n tiene `case_ptr` (ID del caso) y `t_ptr` (paso temporal)

**Explicaciones por transici√≥n:**
- `artifacts/xai/final/risk_explanations.json` - Explicaciones de riesgo por transici√≥n
- `artifacts/xai/final/deltaQ_explanations.json` - Explicaciones contrastivas por transici√≥n
- Cada item tiene `case_id` y `t` (paso temporal)

**Ejemplo de estructura:**
```json
{
  "items": [
    {
      "case_id": 129,
      "t": 10,  // Decisi√≥n en paso 10
      "a_star": 0,
      "a_star_name": "do_nothing",
      "V": -264.77,
      "top_tokens": [...]
    },
    {
      "case_id": 129,
      "t": 11,  // Decisi√≥n en paso 11 (mismo caso, momento diferente)
      "a_star": 1,
      "a_star_name": "contact_headquarters",
      "V": 532.53,
      "top_tokens": [...]
    }
  ]
}
```

---

## 3. Explicaci√≥n detallada de "Doubly Robust"

**¬øQu√© significa "dos formas de contar"?**

Doubly Robust combina **dos m√©todos diferentes** para estimar el valor de una pol√≠tica:

### M√©todo 1: Direct Modeling (Modelado Directo)
- **Qu√© cuenta**: Predice directamente `Q(s,a)` usando el modelo entrenado
- **C√≥mo**: Usa la red neuronal TDQN para predecir `Q(s_t, a_t)` para cada transici√≥n
- **Ventaja**: Si el modelo es bueno, es muy preciso
- **Desventaja**: Si el modelo es malo, el error es grande

**F√≥rmula**: `V_œÄ ‚âà promedio de Q(s,a)` sobre todas las transiciones

### M√©todo 2: Importance Sampling (Muestreo por Importancia)
- **Qu√© cuenta**: Pesa las recompensas observadas seg√∫n qu√© tan probable es que la nueva pol√≠tica hubiera tomado esa acci√≥n vs la pol√≠tica antigua
- **C√≥mo**: Calcula `œÅ = œÄ_nueva(a|s) / œÄ_antigua(a|s)` y multiplica las recompensas por este peso
- **Ventaja**: Funciona bien incluso si el modelo Q es malo
- **Desventaja**: Puede tener alta varianza si las pol√≠ticas son muy diferentes

**F√≥rmula**: `V_œÄ ‚âà promedio de œÅ * r` donde `œÅ` es el peso de importancia

### Doubly Robust: Combinaci√≥n de ambos
- **Qu√© cuenta**: Combina ambos m√©todos de forma inteligente
- **C√≥mo**: `DR = œÅ * (r - Q(s,a)) + Q(s,a)`
  - Si el modelo Q es bueno ‚Üí el t√©rmino `(r - Q(s,a))` es peque√±o ‚Üí DR ‚âà Q(s,a) (usa m√©todo 1)
  - Si el modelo Q es malo pero œÅ es bueno ‚Üí DR ‚âà œÅ * r (usa m√©todo 2)
  - **Es "robusto" porque funciona bien incluso si uno de los dos m√©todos falla**

**C√≥digo:**
- `src/xppm/ope/doubly_robust.py`, l√≠neas 238-240:
```python
# Step-wise DR estimator:
#   DR_t = œÅ_t * (r_t - Q(s_t, a_t)) + V(s_t)
dr_step = rho_trunc * (r - q_sa) + v_s
```

**Resultados guardados en:**
- `artifacts/ope/ope_dr.json` - Contiene `tdqn_dr_mean`, `tdqn_dr_ci95`, etc.

---

## 4. Comparaci√≥n: Figura de Arquitectura vs Implementaci√≥n Real

**An√°lisis de qu√© est√° implementado y qu√© no:**

### ‚úÖ IMPLEMENTADO:

1. **Phase 1 - Data ‚Üí Offline RLSet:**
   - ‚úÖ `01_preprocess_log.py` ‚Üí `clean.parquet`
   - ‚úÖ `02_encode_prefixes.py` ‚Üí `prefixes.npz`
   - ‚úÖ `03_build_mdp_dataset.py` ‚Üí `D_offline.npz`
   - ‚úÖ `01b_validate_and_split.py` ‚Üí `splits.json`

2. **Phase 2 - Training + OPE:**
   - ‚úÖ `04_train_tdqn_offline.py` ‚Üí `Q_theta.ckpt`
   - ‚úÖ `05_run_ope_dr.py` ‚Üí `ope_dr.json`
   - ‚úÖ Behavior policy estimation
   - ‚úÖ Doubly Robust estimator con bootstrap CIs

3. **Phase 3 - XAI:**
   - ‚úÖ `06_explain_policy.py` ‚Üí Risk + DeltaQ explanations
   - ‚úÖ Integrated Gradients attributions
   - ‚úÖ Policy summary (clustering)
   - ‚úÖ `07_fidelity_tests.py` ‚Üí Q-drop, Action-flip, Rank-consistency
   - ‚úÖ `08_distill_policy.py` ‚Üí Decision tree surrogate

4. **Deployment:**
   - ‚úÖ `policy_server.py` (FastAPI)
   - ‚úÖ Policy Guard (OOD detection, uncertainty threshold)
   - ‚úÖ Decision logging

### ‚ö†Ô∏è PARCIALMENTE IMPLEMENTADO:

1. **Counterfactual Rollouts:**
   - ‚ùå Mencionado en la figura (l√≠nea 199) pero **NO implementado**
   - Los fidelity tests solo hacen Q-drop y action-flip, no rollouts completos

2. **Monitoring:**
   - ‚úÖ Scripts de monitoreo existen (`13_compute_monitoring_metrics.py`, `14_detect_drift.py`)
   - ‚ö†Ô∏è Pero el feedback loop completo (l√≠nea 313 de la figura) est√° parcialmente implementado

### ‚ùå NO IMPLEMENTADO:

1. **Experiment Tracking:**
   - La figura muestra W&B/MLflow (l√≠neas 48-52)
   - ‚ö†Ô∏è Configurado pero no siempre usado en todos los scripts

2. **CI/Tests:**
   - La figura muestra pytest (l√≠neas 55-59)
   - ‚úÖ Tests existen pero no todos los componentes est√°n cubiertos

**Voy a crear una figura actualizada que refleje el estado real:**

(Ver archivo `figure-arquitecture-actual.tex`)

---

## 5. ¬øD√≥nde est√° el c√≥digo que explica el plan completo?

**Respuesta:**

El c√≥digo que genera las explicaciones del "plan completo" (no solo una decisi√≥n aislada) est√° en:

### C√≥digo principal:

**`src/xppm/xai/explain_policy.py`** (funci√≥n `explain_policy()`, l√≠neas 255-616):
- **L√≠neas 324-339**: Selecciona m√∫ltiples transiciones por caso (`k_times_per_case`)
- **L√≠neas 350-450**: Genera **Risk explanations** (por qu√© el caso es riesgoso)
- **L√≠neas 450-550**: Genera **DeltaQ explanations** (por qu√© esta ayuda es mejor que otra)
- **L√≠neas 550-600**: Genera **Policy summary** (clustering de estrategias)

**`src/xppm/xai/attributions.py`**:
- **L√≠neas 81-150**: `integrated_gradients_embedding()` - Calcula atribuciones IG
- **L√≠neas 150-250**: `compute_attributions()` - Wrapper que calcula atribuciones para m√∫ltiples targets

**`src/xppm/xai/policy_summary.py`**:
- **L√≠neas 18-62**: `extract_encoder_embeddings()` - Extrae representaciones de estados
- **L√≠neas 100-250**: `summarize_policy()` - Agrupa estados similares en clusters/estrategias

### D√≥nde se guardan los resultados:

**Risk Explanations** (`artifacts/xai/final/risk_explanations.json`):
- Cada item explica **por qu√© el caso es riesgoso** en un momento espec√≠fico
- Contiene `V(s_t)` (valor del estado) y `top_tokens` (tokens m√°s importantes)
- Ejemplo:
```json
{
  "case_id": 129,
  "t": 10,
  "V": -264.77,  // Valor bajo = caso riesgoso
  "top_tokens": [
    {"position": 49, "token_name": "skip_contact", "importance": 7386.17}
  ]
}
```

**DeltaQ Explanations** (`artifacts/xai/final/deltaQ_explanations.json`):
- Cada item explica **por qu√© una acci√≥n es mejor que otra**
- Contiene `delta_q = Q(a*) - Q(a_contrast)` y `top_drivers` (drivers de la diferencia)
- Ejemplo:
```json
{
  "case_id": 552,
  "t": 10,
  "a_star": 1,  // contact_headquarters
  "a_contrast": 0,  // do_nothing
  "delta_q": 797.30,  // Gran diferencia = intervenci√≥n mucho mejor
  "top_drivers": [
    {"position": 48, "token_name": "skip_contact", "importance": 75.11}
  ]
}
```

**Policy Summary** (`artifacts/xai/final/policy_summary.json`):
- Agrupa estados similares en **clusters** (estrategias)
- Cada cluster tiene:
  - `action_distribution`: Qu√© acciones toma en este tipo de estados
  - `mean_v`: Valor promedio del cluster
  - `mean_delta_q`: Diferencia promedio entre acciones
  - `prototypes`: Ejemplos representativos del cluster
- Ejemplo:
```json
{
  "cluster_id": 1,
  "n": 33558,
  "action_distribution": {
    "do_nothing": 0.0,
    "contact_headquarters": 1.0
  },
  "mean_v": 1572.46,
  "mean_delta_q": 934.48,
  "prototypes": [
    {"case_id": 37930, "t": 8, "v": 1601.01}
  ]
}
```

### C√≥mo se explica el "plan completo":

1. **M√∫ltiples momentos**: Las explicaciones se generan para m√∫ltiples pasos temporales (`t=1, t=2, ..., t=L`) del mismo caso
2. **Secuencia de decisiones**: Cada explicaci√≥n muestra por qu√© se decidi√≥ X en el momento t
3. **Estrategia agregada**: El policy summary agrupa estados similares para mostrar "patrones de decisi√≥n" (estrategias)

---

## 6. ¬øQu√© significa "necesitamos muchas formas diferentes de probar las explicaciones"?

**Respuesta:**

Esta frase se refiere a que **una sola prueba de fidelidad no es suficiente** para validar que las explicaciones son confiables. Necesitamos m√∫ltiples pruebas que validen diferentes aspectos.

### ¬øQu√© pruebas de fidelidad tenemos actualmente?

**‚úÖ IMPLEMENTADAS:**

1. **Q-drop** (`src/xppm/xai/fidelity_tests.py`, funci√≥n `_test_q_drop`, l√≠neas 253-525):
   - **Qu√© prueba**: Si quitamos tokens importantes, ¬øbaja m√°s el valor Q que si quitamos tokens aleatorios?
   - **Qu√© valida**: Que las explicaciones realmente identifican qu√© tokens afectan el valor Q
   - **Resultado**: Gap positivo = explicaciones son √∫tiles

2. **Action-flip** (`src/xppm/xai/fidelity_tests.py`, funci√≥n `_test_action_flip`, l√≠neas 528-819):
   - **Qu√© prueba**: Si quitamos tokens importantes, ¬øcambia m√°s la acci√≥n que si quitamos tokens aleatorios?
   - **Qu√© valida**: Que las explicaciones identifican qu√© tokens afectan la decisi√≥n
   - **Resultado**: Flip rate mayor con top-k = explicaciones son √∫tiles

3. **Rank-consistency** (`src/xppm/xai/fidelity_tests.py`, funci√≥n `_test_rank_consistency`, l√≠neas 822-945):
   - **Qu√© prueba**: ¬øEl ranking por Q coincide con el ranking por OPE (proxy)?
   - **Qu√© valida**: Que las explicaciones son consistentes con m√©tricas globales
   - **Resultado**: Correlaci√≥n Spearman/Kendall alta = consistencia

**‚ùå NO IMPLEMENTADAS (pero mencionadas en la literatura):**

4. **Counterfactual Rollouts**:
   - **Qu√© probar√≠a**: Si seguimos la explicaci√≥n y cambiamos los tokens importantes, ¬øqu√© pasa en el futuro?
   - **Qu√© validar√≠a**: Que las explicaciones predicen efectos reales a largo plazo
   - **Estado**: Mencionado en la figura pero NO implementado

5. **Sensitivity Analysis**:
   - **Qu√© probar√≠a**: ¬øQu√© tan robustas son las explicaciones a peque√±os cambios en los inputs?
   - **Qu√© validar√≠a**: Estabilidad de las explicaciones

6. **Human Evaluation**:
   - **Qu√© probar√≠a**: ¬øLos humanos entienden y conf√≠an en las explicaciones?
   - **Qu√© validar√≠a**: Utilidad pr√°ctica de las explicaciones

### ¬øTenemos todas las pruebas necesarias?

**Respuesta corta: NO, pero tenemos las m√°s importantes.**

**Lo que tenemos (3 pruebas):**
- ‚úÖ Q-drop: Valida que las explicaciones afectan el valor Q
- ‚úÖ Action-flip: Valida que las explicaciones afectan las decisiones
- ‚úÖ Rank-consistency: Valida consistencia global

**Lo que falta (pero ser√≠a deseable):**
- ‚ùå Counterfactual rollouts: Validaci√≥n m√°s fuerte de efectos causales
- ‚ùå Sensitivity analysis: Validaci√≥n de robustez
- ‚ùå Human evaluation: Validaci√≥n de utilidad pr√°ctica

**Conclusi√≥n:** Tenemos las pruebas **m√≠nimas necesarias** para publicar, pero idealmente deber√≠amos agregar m√°s pruebas para hacer el paper m√°s fuerte.

**Resultados guardados en:**
- `artifacts/fidelity/fidelity.csv` - Contiene todas las m√©tricas de las 3 pruebas

---

## 7. ¬øD√≥nde est√°n las explicaciones para leerlas?

**Respuesta:**

Las explicaciones est√°n guardadas en archivos JSON en:

### Ubicaciones principales:

1. **`artifacts/xai/final/risk_explanations.json`**
   - Explicaciones de riesgo (por qu√© el caso es riesgoso)
   - Formato: JSON con array de items, cada item tiene:
     - `case_id`: ID del caso
     - `t`: Paso temporal
     - `V`: Valor del estado (bajo = riesgoso)
     - `top_tokens`: Lista de tokens m√°s importantes con su importancia

2. **`artifacts/xai/final/deltaQ_explanations.json`**
   - Explicaciones contrastivas (por qu√© una acci√≥n es mejor que otra)
   - Formato: JSON con array de items, cada item tiene:
     - `case_id`, `t`: Identificaci√≥n
     - `a_star`: Acci√≥n recomendada
     - `a_contrast`: Acci√≥n de contraste
     - `delta_q`: Diferencia en Q-values
     - `top_drivers`: Tokens que explican la diferencia

3. **`artifacts/xai/final/policy_summary.json`**
   - Resumen de la pol√≠tica (clusters/estrategias)
   - Formato: JSON con:
     - `clusters`: Array de clusters, cada uno con:
       - `cluster_id`: ID del cluster
       - `n`: N√∫mero de estados en el cluster
       - `action_distribution`: Distribuci√≥n de acciones
       - `mean_v`, `mean_delta_q`: M√©tricas del cluster
       - `prototypes`: Ejemplos representativos

### C√≥mo leerlas:

**Opci√≥n 1: Ver directamente con `cat` o editor de texto:**
```bash
cat artifacts/xai/final/risk_explanations.json | python -m json.tool | less
```

**Opci√≥n 2: Usar Python para explorar:**
```python
import json

# Leer risk explanations
with open('artifacts/xai/final/risk_explanations.json') as f:
    risk = json.load(f)

# Ver primer caso
print("Primer caso:")
print(f"Case ID: {risk['items'][0]['case_id']}")
print(f"Paso temporal: {risk['items'][0]['t']}")
print(f"Valor V: {risk['items'][0]['V']}")
print(f"Top tokens importantes:")
for token in risk['items'][0]['top_tokens'][:5]:
    print(f"  - {token['token_name']} (posici√≥n {token['position']}): importancia {token['importance']:.2f}")
```

**Opci√≥n 3: Ver en el bundle de deployment:**
- `artifacts/deploy/v1/xai/risk_explanations.json`
- `artifacts/deploy/v1/xai/deltaQ_explanations.json`
- `artifacts/deploy/v1/xai/policy_summary.json`

### Ejemplo de lectura:

**Risk Explanation (caso 129, t=10):**
- **Caso**: 129
- **Momento**: Paso 10
- **Valor V**: -264.77 (negativo = caso riesgoso)
- **Tokens m√°s importantes**:
  1. `skip_contact` (posici√≥n 49): importancia 7386.17
  2. `email_customer` (posici√≥n 47): importancia 321.40
  3. `start_standard` (posici√≥n 41): importancia 241.43

**Interpretaci√≥n**: El caso es riesgoso principalmente porque tiene `skip_contact` al final de la secuencia.

**DeltaQ Explanation (caso 552, t=10):**
- **Caso**: 552
- **Momento**: Paso 10
- **Acci√≥n recomendada**: `contact_headquarters` (Q = 532.53)
- **Acci√≥n de contraste**: `do_nothing` (Q = -264.77)
- **Delta Q**: 797.30 (gran diferencia = intervenci√≥n mucho mejor)
- **Drivers principales**:
  1. `skip_contact` (posici√≥n 48): importancia 75.11
  2. `validate_application` (posici√≥n 49): importancia 35.81

**Interpretaci√≥n**: Contactar HQ es mucho mejor que no hacer nada, principalmente porque el caso tiene `skip_contact` y m√∫ltiples `validate_application`.

---

## Resumen Final

1. **¬øPor qu√© es peligroso practicar con personas reales?**
   - Porque implica tomar decisiones reales mientras se aprende ‚Üí riesgo financiero, √©tico y regulatorio
   - Por eso usamos Offline RL (aprende de datos hist√≥ricos)

2. **¬øQu√© parte del c√≥digo hace decisiones m√∫ltiples?**
   - `src/xppm/data/build_mdp.py` crea m√∫ltiples transiciones por caso
   - `src/xppm/xai/explain_policy.py` genera explicaciones para m√∫ltiples momentos
   - Resultados en `artifacts/xai/final/*.json`

3. **¬øQu√© es Doubly Robust?**
   - Combina dos m√©todos: Direct Modeling (Q-values) + Importance Sampling (pesos)
   - Es "robusto" porque funciona bien incluso si uno de los m√©todos falla
   - C√≥digo en `src/xppm/ope/doubly_robust.py`

4. **¬øQu√© est√° implementado vs la figura?**
   - ‚úÖ Phase 1, 2, 3 principales implementadas
   - ‚ö†Ô∏è Counterfactual rollouts NO implementados
   - ‚ö†Ô∏è Monitoring parcialmente implementado
   - Ver `figure-arquitecture-actual.tex` para detalles

5. **¬øD√≥nde est√° el c√≥digo que explica el plan completo?**
   - `src/xppm/xai/explain_policy.py` genera explicaciones para m√∫ltiples momentos
   - `src/xppm/xai/policy_summary.py` agrupa estrategias
   - Resultados en `artifacts/xai/final/*.json`

6. **¬øQu√© significa "necesitamos muchas formas diferentes de probar"?**
   - Significa que una sola prueba no es suficiente
   - Tenemos 3 pruebas (Q-drop, Action-flip, Rank-consistency)
   - Faltan algunas pruebas avanzadas (rollouts, sensitivity, human eval)

7. **¬øD√≥nde est√°n las explicaciones para leerlas?**
   - `artifacts/xai/final/risk_explanations.json`
   - `artifacts/xai/final/deltaQ_explanations.json`
   - `artifacts/xai/final/policy_summary.json`
