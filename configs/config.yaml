project:
  name: xppm-tdqn
  run_name: "simbank_tdqn_seed42"

# Default dataset — overridden by --dataset CLI arg or configs/datasets/<name>.yaml overlay
dataset_name: "simbank"

# Reproducibility settings
repro:
  seed: 42
  deterministic: true  # Enable deterministic algorithms (may impact performance)

tracking:
  enabled: true
  backend: wandb        # wandb | mlflow | csv
  wandb:
    project: "xppm-tdqn"
    entity: null
    tags: ["offline-rl", "process-mining", "xai"]
  mlflow:
    experiment_name: "xppm-tdqn"
  log_artifacts: true
  log_code: true
  log_config: true

paths:
  # {dataset_name} is substituted automatically by Config.for_dataset()
  data_interim_dir: "data/{dataset_name}/interim"
  data_processed_dir: "data/{dataset_name}/processed"
  artifacts_dir: "artifacts"

data:
  format: pickle      # csv | xes | pickle
  # raw_path and output_clean_path are set in configs/datasets/<name>.yaml
  raw_path: "data/raw/loan_log_[_time_contact_HQ_]_100000_train_normal"
  output_clean_path: "data/{dataset_name}/interim/clean.parquet"

# Schema mapping for SimBank logs (case_nr -> case_id)
schema:
  case_id: "case_nr"    # SimBank uses 'case_nr', map to 'case_id'
  activity: "activity"
  timestamp: "timestamp"
  # Optional mappings if needed:
  # case_id: "case:concept:name"  # for XES
  # activity: "concept:name"
  # timestamp: "time:timestamp"

# Timezone and sorting configuration
time:
  timezone: null        # SimBank timestamps are naive, assume UTC
  output_timezone: "UTC"  # Store in UTC
  sort: true
  drop_duplicates: true

# Legacy column names (for backward compatibility)
case_id_col: "case_id"
activity_col: "activity"
timestamp_col: "timestamp"
resource_col: null
categorical_cols: ["activity"]
numeric_cols: ["amount", "est_quality", "unc_quality", "cum_cost", "elapsed_time"]
drop_cols: []
sort_by_time: true

preprocess:
  out_clean_parquet: "clean.parquet"
  drop_incomplete_cases: true
  impute_missing:
    enabled: true
    strategy: "median"   # median | mean | zero (para numéricas)
  normalize_numeric:
    enabled: true
    method: "zscore"     # zscore | minmax
  filters:
    min_events_per_case: 2
    max_events_per_case: 200

validation_split:
  enabled: true
  split_strategy: "case_id"   # case_id | temporal
  temporal:
    time_col: "timestamp"
    train_end: null
    val_end: null
  ratios:
    train: 0.7
    val: 0.1
    test: 0.2
  out_splits_json: "splits.json"
  schema_checks:
    enabled: true
    schema_file: "configs/schemas/event_log.schema.json"
  range_checks:
    enabled: true
    numeric_ranges:
      amount: [0, 1.0e9]
      elapsed_time: [0, 1.0e9]
  drift_checks:
    enabled: false
    method: "psi"   # psi | ks
    threshold: 0.2

encoding:
  method: "sequence"          # sequence (transformer) | tabular
  max_len: 50                 # maximum prefix length (trunc/pad)
  min_prefix_len: 1           # minimum prefix length to generate
  truncation: left            # left = keep last max_len events, right = keep first max_len
  padding: left               # left or right padding
  vocab:
    min_freq: 1               # minimum frequency to include token
    max_size: null            # maximum vocabulary size (null = no limit)
    add_unk: true             # add UNK token
    add_pad: true             # add PAD token
  fields:
    activity_col: activity    # column name for activities
    resource_col: null         # optional resource column
    use_resource: false        # whether to include resource in encoding
  output:
    prefixes_path: "data/{dataset_name}/interim/prefixes.npz"
    vocab_activity_path: "data/{dataset_name}/interim/vocab_activity.json"
    vocab_resource_path: null  # optional
  # Legacy fields (for backward compatibility)
  prefix_len: 50              # alias for max_len
  pad_token: "<PAD>"
  unk_token: "<UNK>"
  build_prefixes:
    out_prefixes_npy: "prefixes.npz"  # updated to npz
    out_vocab_json: "vocab_activity.json"
  event_embedding:
    activity_vocab_min_freq: 1
    use_time_deltas: true
    time_delta_col: "delta_t"
  state_features:
    include_process_prefix_tokens: true
    include_numeric_case_feats: true

mdp:
  # Activity in the event log that identifies the intervention action.
  # Overridden per dataset in configs/datasets/<name>.yaml.
  behavior_trigger_activity: "contact_headquarters"
  # Decision points for time_contact_HQ intervention
  decision_points:
    mode: "all"  # all = every prefix is a decision point (for time_contact_HQ)
    # Alternative: by_last_activity with specific activities
    # mode: "by_last_activity"
    # activities: ["start_standard", "validate_application", "email_customer", "call_customer"]
  # Actions for time_contact_HQ: do_nothing or contact_headquarters
  actions:
    id2name: ["do_nothing", "contact_headquarters"]
    noop_action: "do_nothing"
  # Action mask: which actions are valid at each decision point
  action_mask:
    default_valid: ["do_nothing"]
    by_last_activity:
      "start_standard": ["do_nothing", "contact_headquarters"]
      "validate_application": ["do_nothing", "contact_headquarters"]
      "email_customer": ["do_nothing", "contact_headquarters"]
      "call_customer": ["do_nothing", "contact_headquarters"]
  # Reward: terminal delayed (outcome at end of case)
  reward:
    type: "terminal_profit_delayed"
    intermediate_reward: 0.0
    terminal_column: "outcome"  # Column in clean.parquet with final profit
  # Output configuration
  output:
    path: "data/{dataset_name}/processed/D_offline.npz"
    schema_path: "configs/schemas/offline_rlset.schema.json"
  # Legacy fields
  build_offline_rlset:
    out_npz: "D_offline.npz"
    store_next_state: true
    store_action_mask: true
    store_metadata: true

training:
  device: "cuda"        # cuda | cpu
  precision: "fp32"     # fp32 | fp16 (si usas amp)
  batch_size: 128       # Reducido de 256 para RTX 3050 6GB
  max_steps: 200000
  eval_every: 5000
  save_every: 10000
  checkpoint_name: "Q_theta.ckpt"

  tdqn:
    gamma: 0.99
    double_dqn: true
    target_update_every: 2000
    grad_clip_norm: 10.0
    learning_rate: 3.0e-4
    weight_decay: 0.0
    optimizer: "adamw"
    lr_scheduler:
      enabled: true
      type: "cosine"     # cosine | step | none
      warmup_steps: 2000

  transformer:
    d_model: 128
    n_heads: 4
    n_layers: 3
    dropout: 0.1
    max_len: 50

  replay:
    type: "offline"         # offline
    capacity: 2000000
    sampling: "uniform"     # uniform | balanced | reweighted
    balanced:
      by_action: true
    reweighting:
      enabled: false
      method: "is_ratio_clip"  # is_ratio_clip | density_ratio
      clip: 10.0

ope:
  enabled: true
  method: "doubly_robust"
  pi_e_temperature: 1.0  # Temperature for π_e softmax policy
  evaluate_heuristic: false  # Set to true to evaluate heuristic baseline
  behavior_model:
    type: "multiclass_logreg"  # multiclass_logreg | mlp
    features: "state"          # state | tabular
    calibrate: true
    batch_size: 1024
    epochs: 1
    learning_rate: 1e-3
    label_smoothing: 0.1
  dr:
    # DR en secuencial: necesitas estimate model + IW; aquí dejas knobs
    bootstrap:
      enabled: true
      n: 200
    clip_importance_weights: 20.0
  heuristic:
    length_threshold: 5  # Prefix length threshold for heuristic
    short_action_id: 0    # Action ID for short prefixes
    long_action_id: 1     # Action ID for long prefixes
  out_json: "ope/ope_dr.json"

xai:
  enabled: true
  checkpoint_path: "artifacts/checkpoints/Q_theta.ckpt"
  seed: 123
  split: "test"
  n_cases: 200
  selection:
    strategy: "random"                      # random | stratified
    by: "reward_bin"                        # reward_bin | stage (only if stratified)
    k_times_per_case: "last"               # "last" | "all" | integer
  methods:
    risk:
      attribution: "integrated_gradients"   # integrated_gradients | gradients | attention
      baseline: "pad"                       # pad | zero_emb
      n_steps_ig: 128                       # IG interpolation steps (increased for better completeness)
      top_k: 10
      target: "Q_star"                      # "V" (max Q) or "Q_star" (Q(s,a*)) - Q_star is smoother for IG
    intervention:
      contrast:
        baseline_action: "NOOP"             # a' para deltaQ
        action_id: 0                        # explicit contrast action ID
        fallback: "first_valid"             # deterministic fallback
        compare_now_vs_delay: true
      delta_q:
        enabled: true
        top_k: 10
  clustering:
    method: "kmeans"
    k: 8
    seed: 123
    n_prototypes: 3
  out_dir: "xai"
  outputs:
    risk_explanations_json: "risk_explanations.json"
    deltaq_explanations_json: "deltaQ_explanations.json"
    attributions_npy: "ig_grad_attributions.npz"
    policy_summary_json: "policy_summary.json"
    selection_json: "explanations_selection.json"

fidelity:
  enabled: true
  seed: 123                    # Random seed for reproducibility
  p_remove: [0.1, 0.2, 0.3, 0.5]  # Removal percentages for Q-drop and action-flip
  n_random: 20                 # Number of random repetitions (use 5 for smoke test)
  n_items: null                # Limit items for smoke test (null = all)
  tests:
    q_drop:
      enabled: true
      deletion: "mask_topk"     # mask_topk | permute_topk | zero_topk
      top_k: 10
    action_flip:
      enabled: true
      top_k: 10
    rank_consistency:
      enabled: true
      metrics: ["spearman", "kendall"]
      compare: ["q_rank", "ope_rank"]
  out_csv: "artifacts/fidelity/fidelity.csv"

distill:
  enabled: true
  seed: 42                   # Random seed for reproducibility
  teacher_checkpoint: "artifacts/models/tdqn/20260209_191903/Q_theta.ckpt"
  method: "viper"            # viper | behavioral_cloning
  surrogate:
    type: "decision_tree"    # decision_tree | rule_list
    max_depth: 5             # Tree depth (4-6 recommended for auditability)
    min_samples_leaf: 50      # Min samples per leaf
    min_samples_split: 100   # Min samples to split
  sample:
    n_states: 2000            # Total states (70% common + 30% high-impact)
    prioritize_by_value: true
    value_quantile: 0.8
  out_dir: "distill"
  outputs:
    tree_pkl: "tree.pkl"
    rules_sql: "rules.sql"

serving:
  enabled: true
  host: "0.0.0.0"
  port: 8000
  guard:
    enabled: true
    uncertainty:
      enabled: true
      method: "ensemble"   # ensemble | mc_dropout | none
      threshold: 0.2
    ood:
      enabled: true
      method: "mahalanobis"  # mahalanobis | density | none
      threshold: 3.0
    fallback_policy: "NOOP"   # o "heuristic"
    human_override: true
  schema:
    file: "configs/schemas/api.schema.json"
